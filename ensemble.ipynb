{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "minmax_scaler = MinMaxScaler()    # For height\n",
    "robust_scaler = RobustScaler()    # For hemoglobin\n",
    "\n",
    "# Normalize each feature differently\n",
    "df['height_normalized'] = minmax_scaler.fit_transform(\n",
    "    df[['height(cm)']])\n",
    "df['hemoglobin_normalized'] = robust_scaler.fit_transform(\n",
    "    df[['hemoglobin']])\n",
    "\n",
    "df['serum creatinine_normalized'] = robust_scaler.fit_transform(\n",
    "    df[['serum creatinine']])\n",
    "df['waist_normalized'] = robust_scaler.fit_transform(df[['waist(cm)']])\n",
    "\n",
    "df['height_hemoglobin'] = (\n",
    "    df['height_normalized'] + df['hemoglobin_normalized']) / 2\n",
    "df['serum creatinine_waist'] = (\n",
    "    df['serum creatinine_normalized'] + df['waist_normalized']) / 2\n",
    "\n",
    "\n",
    "X = df[['height_hemoglobin', 'serum creatinine_normalized', 'HDL']]\n",
    "y = df['smoking']\n",
    "\n",
    "x_train, x_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_model=DecisionTreeClassifier(), n_estimators=100, max_samples=0.8, max_features=0.5):\n",
    "        self.base_model = base_model\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def bootstrap(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, int(\n",
    "            self.max_samples * n_samples), replace=True)\n",
    "        return X.iloc[indices], y.iloc[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.models_ = []\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = self.bootstrap(X, y)\n",
    "            model = self.base_model\n",
    "            model.fit(X_sample, y_sample)\n",
    "            self.models_.append(model)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((self.n_estimators, X.shape[0]))\n",
    "        for i, model in enumerate(self.models_):\n",
    "            predictions[i] = model.predict(X)\n",
    "\n",
    "        # Majority vote\n",
    "        final_predictions = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x.astype(int)).argmax(), axis=0, arr=predictions\n",
    "        )\n",
    "        return final_predictions\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Return hyperparameters for tuning.\"\"\"\n",
    "        return {\"base_model\": self.base_model, \"n_estimators\": self.n_estimators, \"max_samples\": self.max_samples, \"max_features\": self.max_features}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set hyperparameters for tuning.\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Boosting (BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, model, n):\n",
    "        self.model = model\n",
    "        self.n = n\n",
    "        self.models = []\n",
    "        self.model_weights = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        self.models = []\n",
    "        self.model_weights = []\n",
    "        sample_weights = np.ones(len(y)) / len(y)\n",
    "\n",
    "        for _ in range(self.n):\n",
    "            model = self.model()\n",
    "            model.fit(X, y, sample_weight=sample_weights)\n",
    "            predictions = model.predict(X)\n",
    "\n",
    "            error = np.sum(sample_weights * (predictions != y)\n",
    "                           ) / np.sum(sample_weights)\n",
    "\n",
    "            if error == 0:\n",
    "                self.models.append(model)\n",
    "                self.model_weights.append(1)\n",
    "                break\n",
    "\n",
    "            model_weight = 0.5 * np.log((1 - error) / error)\n",
    "            self.models.append(model)\n",
    "            self.model_weights.append(model_weight)\n",
    "\n",
    "            sample_weights = sample_weights * \\\n",
    "                np.exp(-model_weight * y * predictions)\n",
    "            sample_weights /= np.sum(sample_weights)  # Normalize the weights\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        weighted_predictions = np.zeros(len(X))\n",
    "        for model, weight in zip(self.models, self.model_weights):\n",
    "            weighted_predictions += weight * model.predict(X)\n",
    "        # Return the sign of the weighted sum\n",
    "        return np.sign(weighted_predictions)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Return hyperparameters for tuning.\"\"\"\n",
    "        return {\"model\": self.model, \"n\": self.n}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set hyperparameters for tuning.\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest (BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_estimators=100, max_features='auto', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.trees = []\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        return resample(X, y, random_state=self.random_state)\n",
    "\n",
    "    def _select_features(self, X):\n",
    "        if self.max_features == 'auto':\n",
    "            max_features = int(np.sqrt(X.shape[1]))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_features = self.max_features\n",
    "        else:\n",
    "            max_features = X.shape[1]\n",
    "\n",
    "        features = np.random.choice(X.shape[1], max_features, replace=False)\n",
    "        return features\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classes_ = np.unique(y)\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "\n",
    "            features = self._select_features(X_sample)\n",
    "\n",
    "            tree = DecisionTreeClassifier(max_features=len(\n",
    "                features), random_state=self.random_state)\n",
    "            tree.fit(X_sample.iloc[:, features].values, y_sample)\n",
    "\n",
    "            self.trees.append((tree, features))\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((X.shape[0], self.n_estimators), dtype=int)\n",
    "\n",
    "        for i, (tree, features) in enumerate(self.trees):\n",
    "            # Fix: Use .iloc if X is a DataFrame\n",
    "            model_predictions = tree.predict(X.iloc[:, features].values) if hasattr(\n",
    "                X, 'iloc') else tree.predict(X[:, features])\n",
    "\n",
    "            if model_predictions.dtype != int:\n",
    "                model_predictions = np.round(model_predictions).astype(int)\n",
    "\n",
    "            predictions[:, i] = model_predictions\n",
    "\n",
    "        # Use majority voting\n",
    "        majority_vote = np.apply_along_axis(\n",
    "            lambda x: np.bincount(x).argmax(), axis=1, arr=predictions)\n",
    "        return majority_vote\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\"Return hyperparameters for tuning.\"\"\"\n",
    "        return {\"n_estimators\": self.n_estimators, \"max_features\": self.max_features, \"random_state\": self.random_state}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"Set hyperparameters for tuning.\"\"\"\n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Model Accuracy: 0.6490\n",
      "Boosting Model Accuracy: 0.6533\n",
      "Random Forest Accuracy: 0.6526\n"
     ]
    }
   ],
   "source": [
    "bagging_model = Bagging(\n",
    "    base_model=DecisionTreeClassifier(),\n",
    "    n_estimators=50,\n",
    "    max_samples=0.7,\n",
    "    max_features=0.6\n",
    ")\n",
    "\n",
    "boosting_model = Boosting(DecisionTreeClassifier, n=50)\n",
    "\n",
    "rf_classifier = RandomForest(\n",
    "    n_estimators=100, max_features='sqrt', random_state=42)\n",
    "\n",
    "bagging_model.fit(x_train, y_train)\n",
    "boosting_model.fit(x_train, y_train)\n",
    "bagging_predictions = bagging_model.predict(x_test)\n",
    "boosting_predictions = boosting_model.predict(x_test)\n",
    "bagging_accuracy = accuracy_score(y_test, bagging_predictions)\n",
    "boosting_accuracy = accuracy_score(y_test, boosting_predictions)\n",
    "print(f\"Bagging Model Accuracy: {bagging_accuracy:.4f}\")\n",
    "print(f\"Boosting Model Accuracy: {boosting_accuracy:.4f}\")\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "rf_predictions = rf_classifier.predict(x_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging hyperparameter tuning\n",
    "bagging_param_grid = {\n",
    "    'n': [5, 10, 20],\n",
    "    'base_model': [DecisionTreeClassifier(max_depth=d) for d in [5, 10, None]],\n",
    "    'max_samples': [0.5, 0.7, 0.9],\n",
    "    'max_features': [0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "bagging_random_search = RandomizedSearchCV(\n",
    "    estimator=Bagging(),\n",
    "    param_distributions=bagging_param_grid,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "bagging_random_search.fit(x_train, y_train)\n",
    "\n",
    "bagging_random_search.fit(x_train, y_train)\n",
    "best_bagging_model = bagging_random_search.best_estimator_\n",
    "bagging_best_accuracy = bagging_random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "boosting_param_grid = {\n",
    "    'n': [10, 20, 30]  # Number of boosting rounds\n",
    "}\n",
    "\n",
    "boosting_grid_search = GridSearchCV(\n",
    "    estimator=Boosting(DecisionTreeClassifier, n=50),\n",
    "    param_grid=boosting_param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    ")\n",
    "boosting_grid_search.fit(x_train, y_train)\n",
    "best_boosting_model = boosting_grid_search.best_estimator_\n",
    "boosting_best_accuracy = boosting_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noureldeen/venvs/mynewenv/lib/python3.12/site-packages/numpy/ma/core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [5, 10, 20],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [2, 10, 20],\n",
    "}\n",
    "\n",
    "rf_random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForest(random_state=42),\n",
    "    param_distributions=rf_param_grid,\n",
    "    n_iter=10,\n",
    "    scoring='accuracy',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "rf_random_search.fit(x_train, y_train)\n",
    "best_rf_model = rf_random_search.best_estimator_\n",
    "rf_best_accuracy = rf_random_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Bagging Model Accuracy: 0.7211\n",
      "Best Boosting Model Accuracy: 0.6569\n",
      "Best Random Forest Accuracy: 0.6555\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best Bagging Model Accuracy: {bagging_best_accuracy:.4f}\")\n",
    "print(f\"Best Boosting Model Accuracy: {boosting_best_accuracy:.4f}\")\n",
    "print(f\"Best Random Forest Accuracy: {rf_best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Bagging Model Test Accuracy: 0.7217\n",
      "Final Boosting Model Test Accuracy: 0.6603\n",
      "Final Random Forest Test Accuracy: 0.6526\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the best models on the test set\n",
    "bagging_final_predictions = best_bagging_model.predict(x_test)\n",
    "boosting_final_predictions = best_boosting_model.predict(x_test)\n",
    "rf_final_predictions = best_rf_model.predict(x_test)\n",
    "\n",
    "# Compute accuracies\n",
    "bagging_final_accuracy = accuracy_score(y_test, bagging_final_predictions)\n",
    "boosting_final_accuracy = accuracy_score(y_test, boosting_final_predictions)\n",
    "rf_final_accuracy = accuracy_score(y_test, rf_final_predictions)\n",
    "\n",
    "# Display final accuracies\n",
    "print(f\"Final Bagging Model Test Accuracy: {bagging_final_accuracy:.4f}\")\n",
    "print(f\"Final Boosting Model Test Accuracy: {boosting_final_accuracy:.4f}\")\n",
    "print(f\"Final Random Forest Test Accuracy: {rf_final_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Model: Bagging\n",
      "The best model is Bagging(base_model=DecisionTreeClassifier(max_depth=5), max_features=0.9,\n",
      "        max_samples=0.7) with test accuracy of 0.7217\n"
     ]
    }
   ],
   "source": [
    "if bagging_final_accuracy >= boosting_final_accuracy and bagging_final_accuracy >= rf_final_accuracy:\n",
    "    best_model = best_bagging_model\n",
    "    print(\"Selected Model: Bagging\")\n",
    "elif boosting_final_accuracy >= bagging_final_accuracy and boosting_final_accuracy >= rf_final_accuracy:\n",
    "    best_model = best_boosting_model\n",
    "    print(\"Selected Model: Boosting\")\n",
    "else:\n",
    "    best_model = best_rf_model\n",
    "    print(\"Selected Model: Random Forest\")\n",
    "\n",
    "# Save or use the best model\n",
    "print(f\"The best model is {best_model} with test accuracy of {\n",
    "      max(bagging_final_accuracy, boosting_final_accuracy, rf_final_accuracy):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynewenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
